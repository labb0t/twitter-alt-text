{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:24:23.613138Z",
     "start_time": "2020-12-17T23:24:23.607169Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/labbot/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# files\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# text\n",
    "import re\n",
    "import string\n",
    "\n",
    "# nlp\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:20:22.070944Z",
     "start_time": "2020-12-17T23:20:18.569589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2415468, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all the data scraped using twitter_scraping_utils is saved as pickle files; extract and save as a dataframe\n",
    "path = '/Users/labbot/Documents/metis_bootcamp/project05/twitter_data'\n",
    "all_files = glob.glob(path + \"/*.pickle\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    one_df = pd.read_pickle(filename)\n",
    "    li.append(one_df)\n",
    "\n",
    "df_raw = pd.concat(li, axis=0, ignore_index=True)\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:21:19.827215Z",
     "start_time": "2020-12-17T23:21:19.804739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5967, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I wasn't able to exclude gifs in tweepy query, so remove them now\n",
    "df = df[df['media_type'] == 'photo']\n",
    "\n",
    "# filter to only images that contain alt text\n",
    "df = df[df['alt_text'].notnull()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess alt text to use for modeling\n",
    "Glancing through, it (unsurprisingly) looks like the user-created alt text in my corpus will need a fair bit of cleaning to be generalizable. These are the preprocessing steps I'll take:\n",
    "1. remove \"junk\" alt text: pre-populated text like \"discord image\", \"something went wrong\", etc.\n",
    "2. remove photos of text (I want to exclude these because most screen readers have OCR built-in, so my model should focus on other types of images)\n",
    "3. standardize text, removing uppercase and punctuation\n",
    "4. remove the text \"photo of\" or \"picture of\", since alt text best practice is not to include that\n",
    "5. remove any captions that are too long or too short after completing the above steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove records with junk alt text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:21:23.958518Z",
     "start_time": "2020-12-17T23:21:23.946416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User report                                                                                                                                                                                                                                                                                                                                                             262\n",
       "Something went wrong                                                                                                                                                                                                                                                                                                                                                    248\n",
       "Discord Image                                                                                                                                                                                                                                                                                                                                                           150\n",
       "const pluckDeep = key => obj => key.split('.').reduce((accum, key) => accum[key], obj)\\n\\nconst compose = (...fns) => res => fns.reduce((accum, next) => next(accum), res)\\n\\nconst unfold = (f, seed) => {\\n  const go = (f, seed, acc) => {\\n    const res = f(seed)\\n    return res ? go(f, res[1], acc.concat([res[0]])) : acc\\n  }\\n  return go(f, seed, [])\\n}     34\n",
       "Score: 0 - Previous Direction: up                                                                                                                                                                                                                                                                                                                                        25\n",
       "Glitch Cat                                                                                                                                                                                                                                                                                                                                                               21\n",
       "Propuls√© par l'api pokemon.alexonsager                                                                                                                                                                                                                                                                                                                                   17\n",
       "Name: alt_text, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove pre-populated, uninformative captions\n",
    "# which of these appear frequently?\n",
    "df['alt_text'].value_counts().head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. It looks like most of these are junk, only one would be considered \"good\" alt text (brief, descriptive, natural language). Are they all the same image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:22:29.957447Z",
     "start_time": "2020-12-17T23:22:29.937726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>img_url</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>media_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182004</th>\n",
       "      <td>1334180798027718658</td>\n",
       "      <td>2020-12-02 17:01:04</td>\n",
       "      <td>Congrats to $ALGO, the Biggest Green Dildo of ...</td>\n",
       "      <td>https://t.co/NEx3ZGT7fs</td>\n",
       "      <td>http://pbs.twimg.com/media/EoP2XSkW4AAlI7Y.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205463</th>\n",
       "      <td>1333833309126860800</td>\n",
       "      <td>2020-12-01 18:00:16</td>\n",
       "      <td>The Biggest Green Dildo of the Day: $SUSHI, +1...</td>\n",
       "      <td>https://t.co/vvaWqLUxtD</td>\n",
       "      <td>http://pbs.twimg.com/media/EoK6Uy-W4AI8aRj.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317926</th>\n",
       "      <td>1333607122202869762</td>\n",
       "      <td>2020-12-01 03:01:29</td>\n",
       "      <td>Congrats to $QNT, the Biggest Green Dildo of t...</td>\n",
       "      <td>https://t.co/umrX9p6dUD</td>\n",
       "      <td>http://pbs.twimg.com/media/EoHsm-YXMAAp7_h.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361878</th>\n",
       "      <td>1334331809526673408</td>\n",
       "      <td>2020-12-03 03:01:08</td>\n",
       "      <td>Congrats to $SUSHI, the Biggest Green Dildo of...</td>\n",
       "      <td>https://t.co/HuCaO3bKao</td>\n",
       "      <td>http://pbs.twimg.com/media/EoR_tV4W8AAJyqV.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595889</th>\n",
       "      <td>1334528062520758272</td>\n",
       "      <td>2020-12-03 16:00:59</td>\n",
       "      <td>Congrats to $AAVE, the Biggest Green Dildo of ...</td>\n",
       "      <td>https://t.co/RULWz9Y9A4</td>\n",
       "      <td>http://pbs.twimg.com/media/EoUyMxaXcAYKV4Z.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847141</th>\n",
       "      <td>1334377106093051904</td>\n",
       "      <td>2020-12-03 06:01:08</td>\n",
       "      <td>Congrats to $LUNA, the Biggest Green Dildo of ...</td>\n",
       "      <td>https://t.co/8hp5hJsbiw</td>\n",
       "      <td>http://pbs.twimg.com/media/EoSo58RW4AAs45r.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999454</th>\n",
       "      <td>1334361981638365185</td>\n",
       "      <td>2020-12-03 05:01:02</td>\n",
       "      <td>Congrats to $LUNA, the Biggest Green Dildo of ...</td>\n",
       "      <td>https://t.co/HIygT9Skbk</td>\n",
       "      <td>http://pbs.twimg.com/media/EoSbJhSXEAEMCRB.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077086</th>\n",
       "      <td>1334663933802770435</td>\n",
       "      <td>2020-12-04 01:00:53</td>\n",
       "      <td>Congrats to $SC, the Biggest Green Dildo of th...</td>\n",
       "      <td>https://t.co/t66eg4BAsG</td>\n",
       "      <td>http://pbs.twimg.com/media/EoWtxfvXMAIiG3u.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349512</th>\n",
       "      <td>1335026328890986496</td>\n",
       "      <td>2020-12-05 01:00:55</td>\n",
       "      <td>Congrats to $XEM, the Biggest Green Dildo of t...</td>\n",
       "      <td>https://t.co/dkYTU3OXYJ</td>\n",
       "      <td>http://pbs.twimg.com/media/Eob3XsEXcAURKgf.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506586</th>\n",
       "      <td>1335011288058130436</td>\n",
       "      <td>2020-12-05 00:01:09</td>\n",
       "      <td>Congrats to $SC, the Biggest Green Dildo of th...</td>\n",
       "      <td>https://t.co/YG4DqP8aZf</td>\n",
       "      <td>http://pbs.twimg.com/media/EobpsLZXIAc21Ne.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855721</th>\n",
       "      <td>1334724412919214080</td>\n",
       "      <td>2020-12-04 05:01:12</td>\n",
       "      <td>Congrats to $OMG, the Biggest Green Dildo of t...</td>\n",
       "      <td>https://t.co/1YGbiIR08l</td>\n",
       "      <td>http://pbs.twimg.com/media/EoXkx2IXMAExokS.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017091</th>\n",
       "      <td>1334709328532615169</td>\n",
       "      <td>2020-12-04 04:01:16</td>\n",
       "      <td>Congrats to $QNT, the Biggest Green Dildo of t...</td>\n",
       "      <td>https://t.co/FrLE2ysMru</td>\n",
       "      <td>http://pbs.twimg.com/media/EoXXD1KW4AAKOxq.png</td>\n",
       "      <td>Small flowers in a planter on a sunny balcony,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id          created_at  \\\n",
       "182004   1334180798027718658 2020-12-02 17:01:04   \n",
       "205463   1333833309126860800 2020-12-01 18:00:16   \n",
       "317926   1333607122202869762 2020-12-01 03:01:29   \n",
       "361878   1334331809526673408 2020-12-03 03:01:08   \n",
       "595889   1334528062520758272 2020-12-03 16:00:59   \n",
       "847141   1334377106093051904 2020-12-03 06:01:08   \n",
       "999454   1334361981638365185 2020-12-03 05:01:02   \n",
       "1077086  1334663933802770435 2020-12-04 01:00:53   \n",
       "1349512  1335026328890986496 2020-12-05 01:00:55   \n",
       "1506586  1335011288058130436 2020-12-05 00:01:09   \n",
       "1855721  1334724412919214080 2020-12-04 05:01:12   \n",
       "2017091  1334709328532615169 2020-12-04 04:01:16   \n",
       "\n",
       "                                                tweet_text  \\\n",
       "182004   Congrats to $ALGO, the Biggest Green Dildo of ...   \n",
       "205463   The Biggest Green Dildo of the Day: $SUSHI, +1...   \n",
       "317926   Congrats to $QNT, the Biggest Green Dildo of t...   \n",
       "361878   Congrats to $SUSHI, the Biggest Green Dildo of...   \n",
       "595889   Congrats to $AAVE, the Biggest Green Dildo of ...   \n",
       "847141   Congrats to $LUNA, the Biggest Green Dildo of ...   \n",
       "999454   Congrats to $LUNA, the Biggest Green Dildo of ...   \n",
       "1077086  Congrats to $SC, the Biggest Green Dildo of th...   \n",
       "1349512  Congrats to $XEM, the Biggest Green Dildo of t...   \n",
       "1506586  Congrats to $SC, the Biggest Green Dildo of th...   \n",
       "1855721  Congrats to $OMG, the Biggest Green Dildo of t...   \n",
       "2017091  Congrats to $QNT, the Biggest Green Dildo of t...   \n",
       "\n",
       "                       tweet_url  \\\n",
       "182004   https://t.co/NEx3ZGT7fs   \n",
       "205463   https://t.co/vvaWqLUxtD   \n",
       "317926   https://t.co/umrX9p6dUD   \n",
       "361878   https://t.co/HuCaO3bKao   \n",
       "595889   https://t.co/RULWz9Y9A4   \n",
       "847141   https://t.co/8hp5hJsbiw   \n",
       "999454   https://t.co/HIygT9Skbk   \n",
       "1077086  https://t.co/t66eg4BAsG   \n",
       "1349512  https://t.co/dkYTU3OXYJ   \n",
       "1506586  https://t.co/YG4DqP8aZf   \n",
       "1855721  https://t.co/1YGbiIR08l   \n",
       "2017091  https://t.co/FrLE2ysMru   \n",
       "\n",
       "                                                img_url  \\\n",
       "182004   http://pbs.twimg.com/media/EoP2XSkW4AAlI7Y.png   \n",
       "205463   http://pbs.twimg.com/media/EoK6Uy-W4AI8aRj.png   \n",
       "317926   http://pbs.twimg.com/media/EoHsm-YXMAAp7_h.png   \n",
       "361878   http://pbs.twimg.com/media/EoR_tV4W8AAJyqV.png   \n",
       "595889   http://pbs.twimg.com/media/EoUyMxaXcAYKV4Z.png   \n",
       "847141   http://pbs.twimg.com/media/EoSo58RW4AAs45r.png   \n",
       "999454   http://pbs.twimg.com/media/EoSbJhSXEAEMCRB.png   \n",
       "1077086  http://pbs.twimg.com/media/EoWtxfvXMAIiG3u.png   \n",
       "1349512  http://pbs.twimg.com/media/Eob3XsEXcAURKgf.png   \n",
       "1506586  http://pbs.twimg.com/media/EobpsLZXIAc21Ne.png   \n",
       "1855721  http://pbs.twimg.com/media/EoXkx2IXMAExokS.png   \n",
       "2017091  http://pbs.twimg.com/media/EoXXD1KW4AAKOxq.png   \n",
       "\n",
       "                                                  alt_text media_type  \n",
       "182004   Small flowers in a planter on a sunny balcony,...      photo  \n",
       "205463   Small flowers in a planter on a sunny balcony,...      photo  \n",
       "317926   Small flowers in a planter on a sunny balcony,...      photo  \n",
       "361878   Small flowers in a planter on a sunny balcony,...      photo  \n",
       "595889   Small flowers in a planter on a sunny balcony,...      photo  \n",
       "847141   Small flowers in a planter on a sunny balcony,...      photo  \n",
       "999454   Small flowers in a planter on a sunny balcony,...      photo  \n",
       "1077086  Small flowers in a planter on a sunny balcony,...      photo  \n",
       "1349512  Small flowers in a planter on a sunny balcony,...      photo  \n",
       "1506586  Small flowers in a planter on a sunny balcony,...      photo  \n",
       "1855721  Small flowers in a planter on a sunny balcony,...      photo  \n",
       "2017091  Small flowers in a planter on a sunny balcony,...      photo  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['alt_text']=='Small flowers in a planter on a sunny balcony, blossoming.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oooooohkay. Definitely not a good caption; the text is not at all relevant to the image. Not sure why this image has such a mismatched alt text (these tweets look spammy), but hopefully this isn't too common in my dataset.\n",
    "\n",
    "Based on this, I'm going to make the assumption that any caption that's appearing more than once in my dataset is likely junk, and just get rid of all of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:23:25.304396Z",
     "start_time": "2020-12-17T23:23:25.275417Z"
    }
   },
   "outputs": [],
   "source": [
    "vc = df['alt_text'].value_counts()\n",
    "recurrent_alt_texts = list(vc[vc > 1].index)\n",
    "df = df[~df['alt_text'].isin(recurrent_alt_texts)]\n",
    "df.shape\n",
    "\n",
    "# save this current version for topic modeling before removing the images of text\n",
    "# (I want images of text to be included in topic modeling and clustering, but not in the training dataset the captioning model)\n",
    "df.to_pickle('topic_modeling_dataset.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove images of text\n",
    "For V1, I'm going to identify/remove records based on keywords in the alt text that indicate that text is an important part of the image. \n",
    "\n",
    "A future version could use OCR to filter these out, although I think I do want to keep images where text is a small part of the image (eg small caption), so it would ideally be an implementation like `if text area < 15% of total then keep`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:10:43.547500Z",
     "start_time": "2020-12-17T23:10:43.476150Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>img_url</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>media_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>745224</th>\n",
       "      <td>1334519271653842947</td>\n",
       "      <td>2020-12-03 15:26:03</td>\n",
       "      <td>Wrote about Medusa, what a trans story is, and...</td>\n",
       "      <td>https://t.co/RU4LJ3xeh8</td>\n",
       "      <td>http://pbs.twimg.com/media/EoUqNEAXcAI_MqB.jpg</td>\n",
       "      <td>Bronze statue of Medusa, the woman from myth w...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528783</th>\n",
       "      <td>1335009276436340741</td>\n",
       "      <td>2020-12-04 23:53:09</td>\n",
       "      <td>I love [pseudonym]! https://t.co/GzDrynmVYf</td>\n",
       "      <td>https://t.co/GzDrynmVYf</td>\n",
       "      <td>http://pbs.twimg.com/media/Eobn3BtXcAEaEQ_.jpg</td>\n",
       "      <td>An email from Spotify in effort to get me to c...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600823</th>\n",
       "      <td>1334527824342908929</td>\n",
       "      <td>2020-12-03 16:00:02</td>\n",
       "      <td>homagium https://t.co/RjKCPtEaMf</td>\n",
       "      <td>https://t.co/RjKCPtEaMf</td>\n",
       "      <td>http://pbs.twimg.com/media/EoUx-q8VoAUFdSQ.png</td>\n",
       "      <td>homagium</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191102</th>\n",
       "      <td>1334180298117046272</td>\n",
       "      <td>2020-12-02 16:59:05</td>\n",
       "      <td>uncommon complex https://t.co/TBQ9LGRN3v</td>\n",
       "      <td>https://t.co/TBQ9LGRN3v</td>\n",
       "      <td>http://pbs.twimg.com/media/EoP16P-XMAAKDns.png</td>\n",
       "      <td>uncommon complex</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203930</th>\n",
       "      <td>1333805808258322435</td>\n",
       "      <td>2020-12-01 16:11:00</td>\n",
       "      <td>Did You Know...\\n\\n#TriviaTuesday\\n#Monotremes...</td>\n",
       "      <td>https://t.co/uIGv6VThW0</td>\n",
       "      <td>http://pbs.twimg.com/media/EnsFaZiXMAcCI9W.jpg</td>\n",
       "      <td>Monotremes are the only mammals that lay eggs,...</td>\n",
       "      <td>photo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id          created_at  \\\n",
       "745224   1334519271653842947 2020-12-03 15:26:03   \n",
       "1528783  1335009276436340741 2020-12-04 23:53:09   \n",
       "600823   1334527824342908929 2020-12-03 16:00:02   \n",
       "191102   1334180298117046272 2020-12-02 16:59:05   \n",
       "2203930  1333805808258322435 2020-12-01 16:11:00   \n",
       "\n",
       "                                                tweet_text  \\\n",
       "745224   Wrote about Medusa, what a trans story is, and...   \n",
       "1528783        I love [pseudonym]! https://t.co/GzDrynmVYf   \n",
       "600823                    homagium https://t.co/RjKCPtEaMf   \n",
       "191102            uncommon complex https://t.co/TBQ9LGRN3v   \n",
       "2203930  Did You Know...\\n\\n#TriviaTuesday\\n#Monotremes...   \n",
       "\n",
       "                       tweet_url  \\\n",
       "745224   https://t.co/RU4LJ3xeh8   \n",
       "1528783  https://t.co/GzDrynmVYf   \n",
       "600823   https://t.co/RjKCPtEaMf   \n",
       "191102   https://t.co/TBQ9LGRN3v   \n",
       "2203930  https://t.co/uIGv6VThW0   \n",
       "\n",
       "                                                img_url  \\\n",
       "745224   http://pbs.twimg.com/media/EoUqNEAXcAI_MqB.jpg   \n",
       "1528783  http://pbs.twimg.com/media/Eobn3BtXcAEaEQ_.jpg   \n",
       "600823   http://pbs.twimg.com/media/EoUx-q8VoAUFdSQ.png   \n",
       "191102   http://pbs.twimg.com/media/EoP16P-XMAAKDns.png   \n",
       "2203930  http://pbs.twimg.com/media/EnsFaZiXMAcCI9W.jpg   \n",
       "\n",
       "                                                  alt_text media_type  \n",
       "745224   Bronze statue of Medusa, the woman from myth w...      photo  \n",
       "1528783  An email from Spotify in effort to get me to c...      photo  \n",
       "600823                                            homagium      photo  \n",
       "191102                                    uncommon complex      photo  \n",
       "2203930  Monotremes are the only mammals that lay eggs,...      photo  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_keywords = ['reads','says','text','screenshot','\\n','screen shot', 'screencap','screen cap','labeled',\n",
    "                 'username','tweet', 'twitter', 'article', 'receipt',':','-','png','jpg','jpeg','metadata',\n",
    "                 '@','#','.com','success by','bytes','json', 'success from']\n",
    "\n",
    "\n",
    "df = df[~df['alt_text'].apply(lambda r: any([kw in r.lower() for kw in text_keywords]))]\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:10:43.552650Z",
     "start_time": "2020-12-17T23:10:43.549576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3005, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:24:29.157521Z",
     "start_time": "2020-12-17T23:24:29.116313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>img_url</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>media_type</th>\n",
       "      <th>alt_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>993423</th>\n",
       "      <td>1334362559475015683</td>\n",
       "      <td>2020-12-03 05:03:20</td>\n",
       "      <td>@HollytWolf @kayyybearxo @PaulHillierdesu A co...</td>\n",
       "      <td>https://t.co/QFBpKBqnYf</td>\n",
       "      <td>http://pbs.twimg.com/media/EoSbrNSWMAAPA2j.jpg</td>\n",
       "      <td>Nux from Mad Max: Fury Road with chrome paint ...</td>\n",
       "      <td>photo</td>\n",
       "      <td>nux from mad max fury road with chrome paint a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136153</th>\n",
       "      <td>1334698891481059331</td>\n",
       "      <td>2020-12-04 03:19:47</td>\n",
       "      <td>@OrcaTheGreat @mmburton My immediate thought w...</td>\n",
       "      <td>https://t.co/r2T5Hsx70r</td>\n",
       "      <td>http://pbs.twimg.com/media/EoXNaLBW8AM_4Jv.jpg</td>\n",
       "      <td>4 screenshots from 30 Rock of Chris telling Li...</td>\n",
       "      <td>photo</td>\n",
       "      <td>screenshots from  rock of chris telling liz i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id          created_at  \\\n",
       "993423   1334362559475015683 2020-12-03 05:03:20   \n",
       "2136153  1334698891481059331 2020-12-04 03:19:47   \n",
       "\n",
       "                                                tweet_text  \\\n",
       "993423   @HollytWolf @kayyybearxo @PaulHillierdesu A co...   \n",
       "2136153  @OrcaTheGreat @mmburton My immediate thought w...   \n",
       "\n",
       "                       tweet_url  \\\n",
       "993423   https://t.co/QFBpKBqnYf   \n",
       "2136153  https://t.co/r2T5Hsx70r   \n",
       "\n",
       "                                                img_url  \\\n",
       "993423   http://pbs.twimg.com/media/EoSbrNSWMAAPA2j.jpg   \n",
       "2136153  http://pbs.twimg.com/media/EoXNaLBW8AM_4Jv.jpg   \n",
       "\n",
       "                                                  alt_text media_type  \\\n",
       "993423   Nux from Mad Max: Fury Road with chrome paint ...      photo   \n",
       "2136153  4 screenshots from 30 Rock of Chris telling Li...      photo   \n",
       "\n",
       "                                            alt_text_clean  \n",
       "993423   nux from mad max fury road with chrome paint a...  \n",
       "2136153   screenshots from  rock of chris telling liz i...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create field \"alt text clean\" with text all-lowercase and no punctuation\n",
    "df['alt_text_clean'] = df['alt_text'].str.replace('[^a-zA-Z ]+', '').str.lower()\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:26:51.177326Z",
     "start_time": "2020-12-17T23:24:35.098112Z"
    }
   },
   "outputs": [],
   "source": [
    "# create \"alt text cleaner\" with same as above, but also remove any words that don't exist in nltk english corpus\n",
    "# this ends up being too agressive of a removal strategy, getting rid of tons of common words, like \"laptop\"\n",
    "# I also played around with using SpaCy to identify and remove proper nouns, but it wasn't do a good job of identifying them\n",
    "words = nltk.corpus.words.words()\n",
    "df['alt_text_cleaner'] = [' '.join(y for y in x.split() if y in words) for x in df['alt_text_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:26:51.195992Z",
     "start_time": "2020-12-17T23:26:51.179601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>img_url</th>\n",
       "      <th>alt_text</th>\n",
       "      <th>media_type</th>\n",
       "      <th>alt_text_clean</th>\n",
       "      <th>alt_text_cleaner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1369997</th>\n",
       "      <td>1335024385724280832</td>\n",
       "      <td>2020-12-05 00:53:11</td>\n",
       "      <td>@deccybb @michaeljackson my whole house smells...</td>\n",
       "      <td>https://t.co/kcZTGwjOMz</td>\n",
       "      <td>http://pbs.twimg.com/media/Eob1mnjUcAAlSyU.jpg</td>\n",
       "      <td>beef cat</td>\n",
       "      <td>photo</td>\n",
       "      <td>beef cat</td>\n",
       "      <td>beef cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706119</th>\n",
       "      <td>1334521714173218818</td>\n",
       "      <td>2020-12-03 15:35:45</td>\n",
       "      <td>Myself and Adam in lego form are filming, but ...</td>\n",
       "      <td>https://t.co/C8MQjsZIzv</td>\n",
       "      <td>http://pbs.twimg.com/media/EoUsbMCXIAcU6LU.jpg</td>\n",
       "      <td>Two lego figures stand against a tissue paper ...</td>\n",
       "      <td>photo</td>\n",
       "      <td>two lego figures stand against a tissue paper ...</td>\n",
       "      <td>two stand against a tissue paper background an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980271</th>\n",
       "      <td>1334363739588595714</td>\n",
       "      <td>2020-12-03 05:08:01</td>\n",
       "      <td>üéÑChristmassy crittersüéÑ https://t.co/fNINobenUq</td>\n",
       "      <td>https://t.co/fNINobenUq</td>\n",
       "      <td>http://pbs.twimg.com/media/EoSctSPXYAcEwG4.jpg</td>\n",
       "      <td>Black cat, Hamilton, perched on the arm of the...</td>\n",
       "      <td>photo</td>\n",
       "      <td>black cat hamilton perched on the arm of the c...</td>\n",
       "      <td>black cat on the arm of the couch in front of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id          created_at  \\\n",
       "1369997  1335024385724280832 2020-12-05 00:53:11   \n",
       "706119   1334521714173218818 2020-12-03 15:35:45   \n",
       "980271   1334363739588595714 2020-12-03 05:08:01   \n",
       "\n",
       "                                                tweet_text  \\\n",
       "1369997  @deccybb @michaeljackson my whole house smells...   \n",
       "706119   Myself and Adam in lego form are filming, but ...   \n",
       "980271      üéÑChristmassy crittersüéÑ https://t.co/fNINobenUq   \n",
       "\n",
       "                       tweet_url  \\\n",
       "1369997  https://t.co/kcZTGwjOMz   \n",
       "706119   https://t.co/C8MQjsZIzv   \n",
       "980271   https://t.co/fNINobenUq   \n",
       "\n",
       "                                                img_url  \\\n",
       "1369997  http://pbs.twimg.com/media/Eob1mnjUcAAlSyU.jpg   \n",
       "706119   http://pbs.twimg.com/media/EoUsbMCXIAcU6LU.jpg   \n",
       "980271   http://pbs.twimg.com/media/EoSctSPXYAcEwG4.jpg   \n",
       "\n",
       "                                                  alt_text media_type  \\\n",
       "1369997                                           beef cat      photo   \n",
       "706119   Two lego figures stand against a tissue paper ...      photo   \n",
       "980271   Black cat, Hamilton, perched on the arm of the...      photo   \n",
       "\n",
       "                                            alt_text_clean  \\\n",
       "1369997                                           beef cat   \n",
       "706119   two lego figures stand against a tissue paper ...   \n",
       "980271   black cat hamilton perched on the arm of the c...   \n",
       "\n",
       "                                          alt_text_cleaner  \n",
       "1369997                                           beef cat  \n",
       "706119   two stand against a tissue paper background an...  \n",
       "980271   black cat on the arm of the couch in front of ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove 'photo/picture of'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:26:51.211634Z",
     "start_time": "2020-12-17T23:26:51.200019Z"
    }
   },
   "outputs": [],
   "source": [
    "df['alt_text_clean'] = df['alt_text_clean'].str.replace('photo of ','')\n",
    "df['alt_text_clean'] = df['alt_text_clean'].str.replace('picture of ','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove records with alt text that's too long or too short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:26:51.233424Z",
     "start_time": "2020-12-17T23:26:51.213389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4881.000000\n",
       "mean       16.922352\n",
       "std        22.183556\n",
       "min         1.000000\n",
       "25%         5.000000\n",
       "50%        10.000000\n",
       "75%        19.000000\n",
       "max       230.000000\n",
       "Name: alt_text_clean_len, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the current caption length\n",
    "df['alt_text_clean_len'] = df['alt_text_clean'].str.split(\" \").str.len()\n",
    "df['alt_text_clean_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:26:51.241362Z",
     "start_time": "2020-12-17T23:26:51.235021Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2861, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# did a bunch of eyeballing in a spreadsheet, and captions with more than 5/fewer than 36 words seemed to be a reasonable quality cutoff\n",
    "df = df[(df['alt_text_clean_len'] > 5) & (df['alt_text_clean_len'] < 36)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:26:51.254255Z",
     "start_time": "2020-12-17T23:26:51.243099Z"
    }
   },
   "outputs": [],
   "source": [
    "# save dataframe to pickle file for modeling\n",
    "df.to_pickle('twitter_alt_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all images for image caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:11:39.731016Z",
     "start_time": "2020-12-17T23:11:39.728582Z"
    }
   },
   "outputs": [],
   "source": [
    "urls = list(df['img_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:11:39.736121Z",
     "start_time": "2020-12-17T23:11:39.732378Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in urls:\n",
    "     try:\n",
    "        filename = 'twitter_images/'+i[27:46]\n",
    "        response = requests.get(i)\n",
    "        file = open(filename, \"wb\")\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "        if counter % 50 == 0:\n",
    "            print(f'{counter} images downloaded.')\n",
    "        counter +=1\n",
    "     except:\n",
    "         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a random sample of images for clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T23:36:18.603343Z",
     "start_time": "2020-12-17T23:36:18.505025Z"
    }
   },
   "outputs": [],
   "source": [
    "rand550 = df_raw.sample(550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T20:59:31.103622Z",
     "start_time": "2020-12-07T20:58:22.986923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images downloaded.\n",
      "50 images downloaded.\n",
      "100 images downloaded.\n",
      "150 images downloaded.\n",
      "200 images downloaded.\n",
      "250 images downloaded.\n",
      "300 images downloaded.\n",
      "350 images downloaded.\n",
      "400 images downloaded.\n",
      "450 images downloaded.\n",
      "500 images downloaded.\n"
     ]
    }
   ],
   "source": [
    "urls = list(rand550['img_url'])\n",
    "\n",
    "counter = 0\n",
    "for i in urls:\n",
    "     try:\n",
    "        filename = 'clustering_images/'+i[27:46]\n",
    "        response = requests.get(i)\n",
    "        file = open(filename, \"wb\")\n",
    "        file.write(response.content)\n",
    "        file.close()\n",
    "        if counter % 50 == 0:\n",
    "            print(f'{counter} images downloaded.')\n",
    "        counter +=1\n",
    "     except:\n",
    "         pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "305.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
